"use strict";(self.webpackChunkdevelopers=self.webpackChunkdevelopers||[]).push([[563],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>g});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),c=p(a),g=s,u=c["".concat(l,".").concat(g)]||c[g]||m[g]||r;return a?n.createElement(u,i(i({ref:t},d),{},{components:a})):n.createElement(u,i({ref:t},d))}));function g(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=a.length,i=new Array(r);i[0]=c;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:s,i[1]=o;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},8758:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var n=a(7462),s=(a(7294),a(3905));a(8209);const r={},i="7. Building an image classifier",o={unversionedId:"examples/building-an-image-classifier",id:"examples/building-an-image-classifier",title:"7. Building an image classifier",description:"View on GitHub | Run in Google Colab",source:"@site/docs/examples/07-building-an-image-classifier.md",sourceDirName:"examples",slug:"/examples/building-an-image-classifier",permalink:"/docs/examples/building-an-image-classifier",draft:!1,tags:[],version:"current",sidebarPosition:7,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"6. Visualising the collections on a map",permalink:"/docs/examples/visualising-the-collection-on-a-map"},next:{title:"8. Extracting features from text",permalink:"/docs/examples/extracting-features-from-text"}},l={},p=[{value:"7.1 Introduction",id:"71-introduction",level:2},{value:"7.2 Gather an image dataset for fine-tuning",id:"72-gather-an-image-dataset-for-fine-tuning",level:2},{value:"7.3 Splitting the data into train, test, and validation sets",id:"73-splitting-the-data-into-train-test-and-validation-sets",level:2},{value:"7.4 Transforming images for training",id:"74-transforming-images-for-training",level:2},{value:"7.5 Building a <code>Dataset</code> using the processed images",id:"75-building-a-dataset-using-the-processed-images",level:2},{value:"7.6 Creating the model for fine-tuning",id:"76-creating-the-model-for-fine-tuning",level:2},{value:"7.7 Training the model",id:"77-training-the-model",level:2},{value:"7.8 Plotting the training loss",id:"78-plotting-the-training-loss",level:2},{value:"7.9 Inspecting the model&#39;s predictions",id:"79-inspecting-the-models-predictions",level:2}],d={toc:p};function m(e){let{components:t,...a}=e;return(0,s.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"7-building-an-image-classifier"},"7. Building an image classifier"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/wellcomecollection/developers.wellcomecollection.org/tree/HEAD/notebooks/07-building-an-image-classifier.ipynb"},"View on GitHub")," | ",(0,s.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/wellcomecollection/developers.wellcomecollection.org/tree/HEAD/notebooks/07-building-an-image-classifier.ipynb"},"Run in Google Colab")),(0,s.kt)("p",null,"This notebook is going to race through some high-level concepts in machine learning (specifically, fine-tuning a convolutional neural network). However, our focus will remain on demonstrating the practical uses of the Wellcome Collection API. As such, some important ML topics will be covered in less detail than they deserve, and some will be skipped entirely.\nIf you're not already familiar with the basics of ML but want to learn more, I'd recommend exploring ",(0,s.kt)("a",{parentName:"p",href:"https://course.fast.ai/"},"Practical Deep Learning for Coders")," by fast.ai. It describes itself as:"),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.")),(0,s.kt)("p",null,"and will equip you with everything you need to understand and extend the code in this notebook."),(0,s.kt)("h2",{id:"71-introduction"},"7.1 Introduction"),(0,s.kt)("p",null,"Image classification is a classic task in machine learning. By gathering thousands of examples of labelled images from a small number of classes, we can train a model to predict the class for new, unlabelled images."),(0,s.kt)("p",null,"In this notebook, we'll build a model that can classify images based on categories in the collection. We're going to use a pre-trained model, which has already been exposed to on a large collection of labelled images. We'll then re-train it on our own images, taking advantage of the low-level knowledge it gained from its previous task. This is a common technique in machine learning known as fine tuning, or transfer learning."),(0,s.kt)("p",null,"The model we'll fine-tune is called ",(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1512.03385"},"ResNet-50"),", which has been pre-trained on the ",(0,s.kt)("a",{parentName:"p",href:"https://www.image-net.org/"},"ImageNet dataset"),". We'll use the ",(0,s.kt)("inlineCode",{parentName:"p"},"transformers")," library to download the pre-trained model, which uses ",(0,s.kt)("inlineCode",{parentName:"p"},"pytorch")," under the hood."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -U --quiet transformers\n")),(0,s.kt)("h2",{id:"72-gather-an-image-dataset-for-fine-tuning"},"7.2 Gather an image dataset for fine-tuning"),(0,s.kt)("p",null,"To fine-tune a model, we need a dataset of images that are labelled with the classes we want to predict. We'll use the Wellcome Collection API to gather a dataset of images from the collection, filtered by a set of subjects."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import requests\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\n\nfrom PIL import Image\nfrom io import BytesIO\n")),(0,s.kt)("p",null,"The categories we'll use are ",(0,s.kt)("inlineCode",{parentName:"p"},"portrait")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"landscape"),". We want our model to learn the features of the images which contribute to their classification as portraits or landscapes. For example, portraits tend to contain features like hair, eyes, mouths, clothing, and their visual focus is usually concentrated on a single region of the image. Landscapes, however, contain features like trees, buildings, water, and their visual focus is usually spread across the image. Of course, the usual feature that distinguishes a portrait or landscape image is its aspect ratio, but our model won't have access to this information, either during training or when making predictions. We'll use square versions of the images throughout, so that the model has to make its decision based on the content of the image alone."),(0,s.kt)("p",null,"Let's start by defining a function which will download all of the images with a specified genre label."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'base_url = "https://api.wellcomecollection.org/catalogue/v2/"\n\n\ndef get_genre_image_urls(genre):\n    results = {}\n    response = requests.get(\n        base_url + "images",\n        params={\n            "source.genres.label": genre,\n            "pageSize": "100",\n        },\n    ).json()\n\n    progress_bar = tqdm(total=response["totalResults"])\n    results.update(\n        {\n            result["id"]: result["thumbnail"]["url"].replace(\n                "info.json", "full/!400,400/0/default.jpg"\n            )\n            for result in response["results"]\n        }\n    )\n    progress_bar.update(len(response["results"]))\n\n    while "nextPage" in response:\n        response = requests.get(response["nextPage"]).json()\n        results.update(\n            {\n                result["id"]: result["thumbnail"]["url"].replace(\n                    "info.json", "full/!400,400/0/default.jpg"\n                )\n                for result in response["results"]\n            }\n        )\n        progress_bar.update(len(response["results"]))\n\n    progress_bar.close()\n    return results\n')),(0,s.kt)("p",null,"here are all of the image thumbnail urls which have the genre label 'portrait'"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'portrait_urls = get_genre_image_urls("Portrait prints")\n')),(0,s.kt)("p",null,"and here are the landscapes."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'landscape_urls = get_genre_image_urls("Landscape prints")\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"len(portrait_urls), len(landscape_urls)\n")),(0,s.kt)("p",null,"Let's save a local copy of the images to the ",(0,s.kt)("inlineCode",{parentName:"p"},"data/images")," directory. We'll do some processing of the raw images later on, so we'll save the raw versions to the ",(0,s.kt)("inlineCode",{parentName:"p"},"data/images/raw")," directory, with a subdirectory for each genre."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'images_dir = Path("./data/images")\nimages_dir.mkdir(exist_ok=True, parents=True)\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'raw_portraits_dir = images_dir / "raw" / "portraits"\nraw_portraits_dir.mkdir(exist_ok=True, parents=True)\n\nif len(list(raw_portraits_dir.glob("*.jpg"))) == 0:\n    for id, url in tqdm(portrait_urls.items()):\n        response = requests.get(url)\n        image = Image.open(BytesIO(response.content))\n        image.save(raw_portraits_dir / f"{id}.jpg")\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'raw_landscapes_dir = images_dir / "raw" / "landscapes"\nraw_landscapes_dir.mkdir(exist_ok=True, parents=True)\n\nif len(list(raw_landscapes_dir.glob("*.jpg"))) == 0:\n    for id, url in tqdm(landscape_urls.items()):\n        response = requests.get(url)\n        image = Image.open(BytesIO(response.content))\n        image.save(raw_landscapes_dir / f"{id}.jpg")\n')),(0,s.kt)("h2",{id:"73-splitting-the-data-into-train-test-and-validation-sets"},"7.3 Splitting the data into train, test, and validation sets"),(0,s.kt)("p",null,"It's important that our model's accuracy is tested on images it hasn't seen before. To do this, we'll split our dataset into three sets:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Training set"),": used to train the model."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Validation set"),": used to test the model's accuracy during training."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Test set"),": used to test the model's accuracy after training.")),(0,s.kt)("p",null,"We'll use the ",(0,s.kt)("inlineCode",{parentName:"p"},"train_test_split")," function from ",(0,s.kt)("inlineCode",{parentName:"p"},"sklearn")," to split our dataset into a training set and a leftover set. We'll then split that leftover set into a training and a validation set."),(0,s.kt)("p",null,"First, let's set up some directories for our split datasets."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'processed_images_dir = images_dir / "processed"\n\ntrain_dir = processed_images_dir / "train"\n(train_dir / "portraits").mkdir(exist_ok=True, parents=True)\n(train_dir / "landscapes").mkdir(exist_ok=True, parents=True)\n\ntest_dir = processed_images_dir / "test"\n(test_dir / "portraits").mkdir(exist_ok=True, parents=True)\n(test_dir / "landscapes").mkdir(exist_ok=True, parents=True)\n\nval_dir = processed_images_dir / "val"\n(val_dir / "portraits").mkdir(exist_ok=True, parents=True)\n(val_dir / "landscapes").mkdir(exist_ok=True, parents=True)\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.model_selection import train_test_split\n\ntrain_size = 0.7\nval_size = 0.05\ntest_size = 0.25\n\nportraits_paths = list(raw_portraits_dir.glob("*.jpg"))\n\ntrain_portraits_paths, leftover_portraits_paths = train_test_split(\n    portraits_paths, train_size=train_size, random_state=42\n)\n\nval_portraits_paths, test_portraits_paths = train_test_split(\n    leftover_portraits_paths,\n    test_size=test_size / (test_size + val_size),\n    random_state=42,\n)\n\n\nlandscapes_paths = list(raw_landscapes_dir.glob("*.jpg"))\ntrain_landscapes_paths, leftover_landscapes_paths = train_test_split(\n    landscapes_paths, train_size=train_size, random_state=42\n)\n\nval_landscapes_paths, test_landscapes_paths = train_test_split(\n    leftover_landscapes_paths,\n    test_size=test_size / (test_size + val_size),\n    random_state=42,\n)\n')),(0,s.kt)("p",null,"Now that we've set up our directories and split the paths into three sets, we can copy the images into the appropriate directories."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'import shutil\n\nfor path in train_portraits_paths:\n    shutil.copy(path, train_dir / "portraits")\n\nfor path in val_portraits_paths:\n    shutil.copy(path, test_dir / "portraits")\n\nfor path in test_portraits_paths:\n    shutil.copy(path, val_dir / "portraits")\n')),(0,s.kt)("h2",{id:"74-transforming-images-for-training"},"7.4 Transforming images for training"),(0,s.kt)("p",null,"We've created two image classes with visually distinct features (portraits and landscapes) which we should be able to use for training a model. However, our classes are very imbalanced!"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# count the images in /data/images/raw/portraits\n! ls -1 data/images/raw/portraits | wc -l \n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# count the images in /data/images/raw/portraits\n! ls -1 data/images/raw/landscapes | wc -l \n")),(0,s.kt)("p",null,"We have more than 9000 portrait images, and only ~350 landscape images! This is a problem, because our model will learn much more from the portrait images than the landscapes. To limit this effect, we'll use a technique called data augmentation. This involves applying random transformations to the images in our training set, so that the model is exposed to a wider variety of images. We'll use ",(0,s.kt)("inlineCode",{parentName:"p"},"torchvision"),"'s ",(0,s.kt)("inlineCode",{parentName:"p"},"transforms")," module to apply the following transformations:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Random rotation"),": rotates the image by a random angle between -10 and 10 degrees."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Random resize and crop"),": takes a random crop of the image between 50% and 100% of the original size, and resizing it to 400x400 pixels."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Random horizontal flip"),": flips the image horizontally with a 50% probability."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Color jitter"),": randomly changes the brightness, contrast, saturation, and hue of the image by 20%.")),(0,s.kt)("p",null,"We'll apply these transformations to the images in our landscapes training set 10 times, and then save the transformed images alongside the originals."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from torchvision import transforms\n\ntransform = transforms.Compose(\n    [\n        transforms.RandomRotation(10),\n        transforms.RandomResizedCrop(400, scale=(0.5, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(\n            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2\n        ),\n    ]\n)\n\nfor file in tqdm(train_landscapes_paths):\n    image = Image.open(file)\n    for i in range(10):\n        transformed_image = transform(image)\n        transformed_image.save(\n            train_dir / "landscapes" / f"{file.stem}-{i}.jpg"\n        )\n\nfor file in val_landscapes_paths:\n    image = Image.open(file)\n    for i in range(10):\n        transformed_image = transform(image)\n        transformed_image.save(val_dir / "landscapes" / f"{file.stem}-{i}.jpg")\n\nfor file in test_landscapes_paths:\n    image = Image.open(file)\n    for i in range(10):\n        transformed_image = transform(image)\n        transformed_image.save(test_dir / "landscapes" / f"{file.stem}-{i}.jpg")\n')),(0,s.kt)("h2",{id:"75-building-a-dataset-using-the-processed-images"},"7.5 Building a ",(0,s.kt)("inlineCode",{parentName:"h2"},"Dataset")," using the processed images"),(0,s.kt)("p",null,"Pytorch uses the ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataset")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataloader")," objects to load images into a model for training. When the model asks for a batch of images, the ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataloader")," will load them from the ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataset")," and apply any transformations that have been specified. "),(0,s.kt)("p",null,"The ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataset")," here keeps track of all of the image paths, and when ",(0,s.kt)("inlineCode",{parentName:"p"},"__getitem__()")," is called, loads the image and transforms it into a 224x224 ",(0,s.kt)("inlineCode",{parentName:"p"},"tensor"),", along with a tensor representing the class of the image. The ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataloader")," then batches these tensors together and feeds them to the model."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from transformers import ResNetForImageClassification\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'class ImageDataset(Dataset):\n    def __init__(self, data_dir):\n        self.data_dir = data_dir\n        self.classes = sorted(\n            [\n                image_class.name\n                for image_class in data_dir.glob("*")\n                if not image_class.name.startswith(".")\n            ]\n        )\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n            ]\n        )\n\n    def get_target_tensor(self, image_path):\n        target_tensor = torch.zeros(len(self.classes))\n        target_tensor[self.classes.index(image_path.parent.name)] = 1\n        return target_tensor\n\n    def __len__(self):\n        return len(list(self.data_dir.glob("**/*.jpg")))\n\n    def __getitem__(self, idx):\n        image_path = list(self.data_dir.glob("**/*.jpg"))[idx]\n        image = Image.open(image_path)\n        image_tensor = self.image_transforms(image)\n        target_tensor = self.get_target_tensor(image_path)\n        return image_tensor, target_tensor\n')),(0,s.kt)("p",null,"Let's create a ",(0,s.kt)("inlineCode",{parentName:"p"},"Dataset")," and ",(0,s.kt)("inlineCode",{parentName:"p"},"DataLoader")," for each of our training, validation, and test sets."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"train_dataset = ImageDataset(train_dir)\nval_dataset = ImageDataset(val_dir)\ntest_dataset = ImageDataset(test_dir)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"batch_size = 32\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=0,\n    shuffle=True,\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    num_workers=0,\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    num_workers=0,\n)\n")),(0,s.kt)("h2",{id:"76-creating-the-model-for-fine-tuning"},"7.6 Creating the model for fine-tuning"),(0,s.kt)("p",null,"Now we can create the model we'll use for fine-tuning. We'll use a pre-trained ",(0,s.kt)("inlineCode",{parentName:"p"},"resnet50")," model from ",(0,s.kt)("inlineCode",{parentName:"p"},"torchvision"),". Because this network has been trained on the ImageNet dataset, its last layer is 1000 neurons wide, with each one representing a different class. We'll replace this layer with a new layer which has two outputs, one for each of our new classes."),(0,s.kt)("p",null,"We'll also freeze all of the layers except the final layer, so that only the final layer is trained during fine-tuning. This has the dual effect of "),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"retaining all of the lower-level features that the model learned during its training on ImageNet, and "),(0,s.kt)("li",{parentName:"ul"},"massively speeding up the transfer learning process, because we only have to train a single layer.")),(0,s.kt)("p",null,"We'll use the ",(0,s.kt)("inlineCode",{parentName:"p"},"Adam")," optimizer to train the model, and the ",(0,s.kt)("inlineCode",{parentName:"p"},"CrossEntropyLoss")," function to calculate the loss at each step."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from torch.nn.functional import cross_entropy\nfrom torch import nn\n\n\nmodel_name = "microsoft/resnet-50"\n\nmodel = ResNetForImageClassification.from_pretrained(model_name)\n\n# replace the original classifier head with a linear layer,\n# squashing the output down to 2 classes\nmodel.classifier = nn.Sequential(\n    nn.Flatten(start_dim=1, end_dim=-1),\n    nn.Linear(in_features=2048, out_features=2, bias=True),\n)\n\n# freeze all the layers except the classifier head\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n")),(0,s.kt)("h2",{id:"77-training-the-model"},"7.7 Training the model"),(0,s.kt)("p",null,"For each batch, we'll run the image tensors through the model, calculate the loss, and then backpropagate the loss through the model to update the weights. At the end of each epoch, we'll calculate the accuracy of the model's classifications against the validation set. "),(0,s.kt)("p",null,"For now, we'll just train the model for one epoch. We'll display the training and validation losses and accuracies, and keep track of them so that we can plot the whole thing later on."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'n_epochs = 1\n\ntrain_losses = []\n\nfor epoch in range(n_epochs):\n    model.train()\n    train_progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch}")\n    for image_tensors, targets in train_progress_bar:\n        optimiser.zero_grad()\n\n        predicted_classes = model.forward(image_tensors)\n        loss = cross_entropy(predicted_classes.logits, targets)\n\n        loss.backward()\n        optimiser.step()\n\n        train_losses.append(loss.item())\n        train_loss = sum(train_losses[-10:]) / len(train_losses[-10:])\n        train_progress_bar.set_description(f"Train loss: {train_loss:.3f}")\n\n    val_losses = []\n    val_progress_bar = tqdm(val_dataloader, desc=f"Epoch {epoch}")\n    for image_tensors, targets in val_progress_bar:\n        model.eval()\n        predicted_classes = model.forward(image_tensors)\n        loss = cross_entropy(predicted_classes.logits, targets)\n        val_losses.append(loss.item())\n        mean_val_loss = sum(val_losses) / len(val_losses)\n        val_progress_bar.set_description(\n            f"Train loss: {train_loss:.3f} Validation loss: {mean_val_loss:.3f}"\n        )\n')),(0,s.kt)("h2",{id:"78-plotting-the-training-loss"},"7.8 Plotting the training loss"),(0,s.kt)("p",null,"Let's have a look at the rolling mean of the training loss."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ntrain_losses_df = pd.DataFrame(train_losses, columns=["train_loss"])\nrolling_train_losses_df = train_losses_df.rolling(10).mean()\nrolling_train_losses_df.plot()\n')),(0,s.kt)("p",null,"We can also use our newly trained model in evaluation mode to make predictions on the test set. We'll work our way through the test set, and for each image, we'll predict the class and compare it to the actual class. We'll then calculate the accuracy of the model's predictions, and display the results as a classification report and a confusion matrix."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"\nmodel.eval()\n\ny_true = []\ny_predicted = []\n\nfor image_tensors, targets in tqdm(test_dataloader):\n    predicted_classes = model.forward(image_tensors)\n    predicted_labels = predicted_classes.logits.argmax(dim=1)\n    y_true.extend(targets.argmax(dim=1).tolist())\n    y_predicted.extend(predicted_labels.tolist())\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.metrics import classification_report, confusion_matrix\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"print(classification_report(y_true, y_predicted))\n")),(0,s.kt)("p",null,"With a minimal amount of training data, and without any custom feature engineering, we were able to train a model that can classify images of portraits and landscapes with 98% accuracy! This is pretty remarkable, considering the fact that we didn't have to specify any rules or features for telling the computer how to tell the difference between the classes."),(0,s.kt)("p",null,"We can visualise the classification metrics differently, using a confusion matrix. We can use it to see which classes are getting confused, i.e., whether the model is misclassifying portraits as landscapes or vice versa."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'cm = confusion_matrix(y_true, y_predicted)\nsns.heatmap(cm, annot=True, fmt="d")\n\nplt.xlabel("Predicted")\nplt.xticks([0.5, 1.5], ["landscape", "portrait"])\n\nplt.ylabel("True")\nplt.yticks([0.5, 1.5], ["landscape", "portrait"], rotation=0)\n\nplt.show()\n')),(0,s.kt)("h2",{id:"79-inspecting-the-models-predictions"},"7.9 Inspecting the model's predictions"),(0,s.kt)("p",null,"Let's have a look at some random images from the test set, and see how the model classifies them."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import random\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# get a real image from the test set and run it through the model\ndataset = test_dataloader.dataset\nimage_tensor, target_tensor = random.choice(dataset)\npredicted_classes = model.forward(image_tensor.unsqueeze(0))\nprediction = predicted_classes.logits\n\n# show the image\nplt.imshow(image_tensor.permute(1, 2, 0))\nplt.show()\n\n# print the probabilities and the predicted class\nprint("Probabilities:", prediction.softmax(dim=1).squeeze().tolist())\nprint("True:", dataset.classes[target_tensor.argmax().item()])\nprint("Predicted:", dataset.classes[prediction.argmax().item()])\n')),(0,s.kt)("p",null,"By inspecting the predicted probabilities for each class and the true labels, we can look directly at the images where the model is most/least confident in its predictions, and, most interestingly, the images which it gets most wrong!"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# find the images in the test set which the model has misclassified\n\nconfused_indexes = [\n    index\n    for index, (y_true, y_pred) in enumerate(zip(y_true, y_predicted))\n    if y_true != y_pred\n]\n\nlen(confused_indexes)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'# show the images where the model got confused\n\nfor index in confused_indexes:\n    image_tensor, true_label = test_dataset[index]\n    prediction = model.forward(image_tensor.unsqueeze(0)).logits\n    predicted_label = prediction.argmax(dim=1)\n\n    # show the image\n    plt.imshow(image_tensor.permute(1, 2, 0))\n    plt.show()\n\n    # show the predicted class\n    classes = {0: "landscape", 1: "portrait"}\n\n    print("Probabilities:", prediction.softmax(dim=1).squeeze().tolist())\n    print("True:", classes[true_label.argmax().item()])\n    print("Predicted:", classes[predicted_label.item()])\n')),(0,s.kt)("p",null,"A lot of those misclassifications are quite understandable - I might have guessed that some of those portraits were landscapes, too. We could use these misclassifications to improve our model, but that's beyond the scope of this notebook."),(0,s.kt)("h1",{id:"exercises"},"Exercises"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"Adapt this notebook to classify a different pair of subjects"),(0,s.kt)("li",{parentName:"ol"},"Adapt the code to classify more than two subjects!"),(0,s.kt)("li",{parentName:"ol"},"Try using a different pre-trained model from ",(0,s.kt)("inlineCode",{parentName:"li"},"torchvision.models")),(0,s.kt)("li",{parentName:"ol"},"Balance the class weights used by the optimiser by using ",(0,s.kt)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html"},"sklearn's ",(0,s.kt)("inlineCode",{parentName:"a"},"compute_class_weight")," function"))))}m.isMDXComponent=!0},8209:(e,t,a)=>{a(7294)}}]);