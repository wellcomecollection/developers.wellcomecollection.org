"use strict";(self.webpackChunkdevelopers=self.webpackChunkdevelopers||[]).push([[246],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=d(n),m=r,h=c["".concat(l,".").concat(m)]||c[m]||u[m]||o;return n?a.createElement(h,i(i({ref:t},p),{},{components:n})):a.createElement(h,i({ref:t},p))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var d=2;d<o;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},6658:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var a=n(7462),r=(n(7294),n(3905));n(8209);const o={},i="8. Extracting features from text",s={unversionedId:"examples/extracting-features-from-text",id:"examples/extracting-features-from-text",title:"8. Extracting features from text",description:"View on GitHub | Run in Google Colab",source:"@site/docs/examples/08-extracting-features-from-text.md",sourceDirName:"examples",slug:"/examples/extracting-features-from-text",permalink:"/docs/examples/extracting-features-from-text",draft:!1,tags:[],version:"current",sidebarPosition:8,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"7. Building an image classifier",permalink:"/docs/examples/building-an-image-classifier"},next:{title:"9. Building a recommender system for subjects",permalink:"/docs/examples/building-a-recommender-system-for-subjects"}},l={},d=[{value:"8.2 Text embedding models",id:"82-text-embedding-models",level:2},{value:"8.3 Visualising the embeddings",id:"83-visualising-the-embeddings",level:2},{value:"8.4 Interactive visualisations",id:"84-interactive-visualisations",level:2},{value:"8.5 Clustering",id:"85-clustering",level:2},{value:"8.6 Visualising the clusters",id:"86-visualising-the-clusters",level:2},{value:"8.7 3D Visualisation",id:"87-3d-visualisation",level:2},{value:"Exercises",id:"exercises",level:2}],p={toc:d};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"8-extracting-features-from-text"},"8. Extracting features from text"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/wellcomecollection/developers.wellcomecollection.org/tree/HEAD/notebooks/08-extracting-features-from-text.ipynb"},"View on GitHub")," | ",(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/wellcomecollection/developers.wellcomecollection.org/tree/HEAD/notebooks/08-extracting-features-from-text.ipynb"},"Run in Google Colab")),(0,r.kt)("p",null,"In the last notebook, we saw that using a pre-trained network allowed us to extract features from images, and train a classifier for new categories on top of those features. We can do the same thing with text, using a pre-trained network to extract features from text. In this notebook, we'll use those features the visualise the similarities and differences between works in the collection, and try to find clusters of related material."),(0,r.kt)("p",null,"First, we need to install a few packages. We'll use ",(0,r.kt)("inlineCode",{parentName:"p"},"sentence-transformers")," to manage our pre-trained language models, and ",(0,r.kt)("inlineCode",{parentName:"p"},"umap-learn")," to compress our high-dimensional features, and ",(0,r.kt)("inlineCode",{parentName:"p"},"plotly")," to visualise the results."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"! pip install -U --quiet sentence-transformers umap-learn plotly\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from pathlib import Path\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.auto import tqdm\nfrom umap import UMAP\nimport gzip\nimport io\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport requests\n")),(0,r.kt)("p",null,"##\xa08.1 Building a dataset "),(0,r.kt)("p",null,"We'll use the works snapshot in this exercise (as explained in notebook 4), but this data could just as easily be fetched from the API. The following code is the same as in notebook 4."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'snapshot_url = "https://data.wellcomecollection.org/catalogue/v2/works.json.gz"\n\ndata_dir = Path("./data").resolve()\ndata_dir.mkdir(exist_ok=True)\n\nfile_name = Path(snapshot_url).parts[-1]\nzipped_path = data_dir / file_name\nunzipped_path = zipped_path.with_suffix("")\n\n\nif not unzipped_path.exists():\n    if not zipped_path.exists():\n        r = requests.get(snapshot_url, stream=True)\n        download_progress_bar = tqdm(\n            unit="B",\n            total=int(r.headers["Content-Length"]),\n            desc=f"downloading {file_name}",\n        )\n        with open(zipped_path, "wb") as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n                    download_progress_bar.update(len(chunk))\n\n    with gzip.open(zipped_path, "rb") as f_in:\n        unzip_progress_bar = tqdm(\n            unit="B",\n            total=f_in.seek(0, io.SEEK_END),\n            desc=f"unzipping {file_name}",\n        )\n        with open(unzipped_path, "wb") as f_out:\n            for line in f_in:\n                f_out.write(line)\n                unzip_progress_bar.update(len(line))\n\nif zipped_path.exists():\n    zipped_path.unlink()\n')),(0,r.kt)("p",null,"Now we can start building a dataset of work titles. Let's select 50,000 random works from the collection, and then extract their title text into a list of strings."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"n_works = sum(1 for line in unzipped_path.open())\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'random_indexes = np.random.choice(n_works, 50_000, replace=False)\nwith open(unzipped_path, "r") as f:\n    works = []\n    for i, line in enumerate(f):\n        if i in random_indexes:\n            works.append(json.loads(line))\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'titles = [work["title"] for work in works]\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"titles[:5]\n")),(0,r.kt)("h2",{id:"82-text-embedding-models"},"8.2 Text embedding models"),(0,r.kt)("p",null,"Now that we have a dataset to work with, we can download the weights for a pretrained feature-extraction model. We're going to use the small but powerful ",(0,r.kt)("inlineCode",{parentName:"p"},"all-MiniLM-L6-v2")," model (see the ",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"},"docs on huggingface"),", or a ",(0,r.kt)("a",{parentName:"p",href:"https://www.sbert.net/docs/pretrained_models.html#model-overview"},"table comparing its performance to other models in the sentence transformers docs"),")."),(0,r.kt)("p",null,"We'll save the weights locally to ",(0,r.kt)("inlineCode",{parentName:"p"},"./data/models"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'model_name = "all-MiniLM-L6-v2"\nmodel = SentenceTransformer(model_name, cache_folder="./data/models")\n')),(0,r.kt)("p",null,"We can use our model to extract features from our text. The ",(0,r.kt)("inlineCode",{parentName:"p"},"model.encode()")," method takes a list of strings, and returns a list of 384-dimensional vectors. These features behave similarly to the image features we extracted in the last notebook. "),(0,r.kt)("p",null,"For example, the sentence"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"the cat sat on the mat")),(0,r.kt)("p",null,"should be very similar (ie have a small distance from) the sentence"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"a feline sits above the rug")),(0,r.kt)("p",null,"despite having few words in common."),(0,r.kt)("p",null,"Both should have a much larger distance from the sentence"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"i hate this film")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'texts = [\n    "the cat sat on the mat",\n    "a feline sits above the rug",\n    "i hate this film",\n]\n\nembeddings = model.encode(texts)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"embeddings.shape\n")),(0,r.kt)("p",null,"We can calculate the similarity of embeddings using the cosine distances between them."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from scipy.spatial.distance import cdist\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'cdist(embeddings, embeddings, metric="cosine")\n')),(0,r.kt)("p",null,"The diagonal here represents the distance from each sentence to itself, while the off-diagonal values represent the distance between each pair of sentences. We can see that the first two sentences are very similar (distance ~= 0.4), while the third is very different (distance ~= 1)."),(0,r.kt)("p",null,"We can run the same encoding process for every title in our dataset:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"title_vectors = np.array([model.encode(title) for title in tqdm(titles)])\n")),(0,r.kt)("p",null,"Again, we should expect that very similar titles will produce similar embeddings, while very different titles will produce very different embeddings."),(0,r.kt)("h2",{id:"83-visualising-the-embeddings"},"8.3 Visualising the embeddings"),(0,r.kt)("p",null,"The embeddings we've produced are 384-dimensional - too many to visualise directly. While the 384 dimensions give the model lots of room to express the differences between sentences, it's very hard to visualise more than 3 dimensions at a time. To get around this, we can use a ",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Dimensionality_reduction"},"dimensionality reduction")," technique to compress our 384-dimensional vectors into 3 dimensions. We'll use the ",(0,r.kt)("a",{parentName:"p",href:"https://umap-learn.readthedocs.io/en/latest/"},"UMAP")," algorithm to compress our initial vectors down to 2 dimensions so that they can be scattered on a 2D plot."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"dimension_reducer = UMAP(n_components=2, random_state=42)\ntitle_embeddings_2d = dimension_reducer.fit_transform(title_vectors)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'plt.figure(figsize=(20, 20))\nplt.scatter(\n    title_embeddings_2d[:, 0], title_embeddings_2d[:, 1], alpha=0.2, c="k", s=5\n)\nplt.show()\n')),(0,r.kt)("p",null,"By trying to preserve the distances between points, UMAP can give us a good idea of the relationships between our titles. We can see clusters of points across the plot, indicating that there are a few groups of similar titles, distinct from the rest of our dataset"),(0,r.kt)("h2",{id:"84-interactive-visualisations"},"8.4 Interactive visualisations"),(0,r.kt)("p",null,"We can visualise this data in a more interactive way using ",(0,r.kt)("a",{parentName:"p",href:"https://plotly.com/python/"},"plotly"),". Plotly is a powerful plotting library that allows us to create interactive plots that can be embedded in web pages. We can use plotly to create a scatter plot of our data, and then add a hover effect that shows the title of each work when we hover over it."),(0,r.kt)("p",null,"N.B. You won't be able to see this if you're reading the markdown version of this notebook, or viewing it on GitHub. You'll need to run the notebook yourself to see the interactive plot."),(0,r.kt)("p",null,"We'll start by loading our 2d embeddings into a dataframe, along with the original titles."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df = pd.DataFrame(title_embeddings_2d, columns=["x", "y"])\ndf["title"] = titles\n')),(0,r.kt)("p",null,"We'll then use plotly to create a scatter plot of our data, with the the ",(0,r.kt)("inlineCode",{parentName:"p"},"hover_data")," parameter set to add the title of each work when the user hovers over it."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'fig = px.scatter(\n    df, x="x", y="y", hover_data=["title"], width=1000, height=1000\n)\nfig.update_traces(marker=dict(size=5, opacity=0.2, color="grey"))\nfig.update_layout(\n    paper_bgcolor="white",\n    plot_bgcolor="white",\n    xaxis=dict(gridcolor="rgb(220, 220, 220)", showgrid=True),\n    yaxis=dict(gridcolor="rgb(220, 220, 220)", showgrid=True),\n)\n')),(0,r.kt)("p",null,"As expected, similar titles have been placed in similar regions of the space! We can see that the model has learned to distinguish between titles that are similar in meaning, but different in wording, and titles that are completely different."),(0,r.kt)("h2",{id:"85-clustering"},"8.5 Clustering"),(0,r.kt)("p",null,"We can use the features we've extracted to cluster our works into groups of similar titles. We'll use the k-means algorithm to cluster our works into 50 groups. We'll then add the cluster labels to our dataframe, and use plotly to colour the points in our plot by cluster."),(0,r.kt)("p",null,"N.B. Many other clustering algorithms are available, and might yield better results! If you're running this notebook yourself, try switching the clusterer to use the ",(0,r.kt)("inlineCode",{parentName:"p"},"OPTICS")," algorithm instead, taking advantage of the fact that it doesn't require us to specify the number of clusters in advance."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.cluster import OPTICS, KMeans\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"clusterer = KMeans(n_clusters=50)\n\n# clusterer = OPTICS(min_samples=10, xi=0.01, min_cluster_size=0.001)\n")),(0,r.kt)("p",null,"Note here that we're finding our clusters in our original, 384-dimensional space, instead of our reduced 2d space. This allows us to retain all of the complexity of our original embeddings, and find clusters that are more meaningful than those we'd find in our reduced space."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"clusters = clusterer.fit_predict(title_vectors)\n")),(0,r.kt)("p",null,"Let's add those cluster labels to our dataframe."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import pandas as pd\n\ndf = pd.DataFrame(\n    {\n        "title": titles,\n        "cluster": clusters,\n        "x": title_embeddings_2d[:, 0],\n        "y": title_embeddings_2d[:, 1],\n    }\n)\n\ndf.head()\n')),(0,r.kt)("p",null,"And look at the number of titles which have been added to each bucket"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df["cluster"].value_counts()\n')),(0,r.kt)("h2",{id:"86-visualising-the-clusters"},"8.6 Visualising the clusters"),(0,r.kt)("p",null,"Remember, we've found our clusters in our original, 384-dimensional space, but we're visualising them in our reduced 2d space. This means that we might not see all of the complexity of our original embeddings in our reduced-space visualisation, so the clusters might look less coherent when we plot them!"),(0,r.kt)("p",null,"N.B. You won't be able to see this if you're reading the markdown version of this notebook, or viewing it on GitHub. You'll need to run the notebook yourself to see the interactive plot."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'fig = px.scatter(\n    df,\n    x="x",\n    y="y",\n    color="cluster",\n    hover_data=["title"],\n    width=1000,\n    height=1000,\n)\n\nfig.update_layout(\n    paper_bgcolor="white",\n    plot_bgcolor="white",\n    xaxis=dict(gridcolor="rgb(220, 220, 220)", showgrid=True),\n    yaxis=dict(gridcolor="rgb(220, 220, 220)", showgrid=True),\n)\n')),(0,r.kt)("h2",{id:"87-3d-visualisation"},"8.7 3D Visualisation"),(0,r.kt)("p",null,"We can also use plotly to build 3D interactive scatter plots, which can be rotated and zoomed to explore the data. We'll use roughly the same code as before, but use a UMAP model with ",(0,r.kt)("inlineCode",{parentName:"p"},"n_components")," set to ",(0,r.kt)("inlineCode",{parentName:"p"},"3")," to reduce our embeddings to 3 dimensions instead of 2. "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"\ndimension_reducer = UMAP(n_components=3, random_state=42, n_jobs=-1)\ntitle_embeddings_3d = dimension_reducer.fit_transform(title_vectors)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df = pd.DataFrame(\n    {\n        "title": titles,\n        "cluster": clusters,\n        "x": title_embeddings_3d[:, 0],\n        "y": title_embeddings_3d[:, 1],\n        "z": title_embeddings_3d[:, 2],\n    }\n)\n')),(0,r.kt)("p",null,"N.B. You won't be able to see this if you're reading the markdown version of this notebook, or viewing it on GitHub. You'll need to run the notebook yourself to see the interactive plot."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'fig = px.scatter_3d(\n    df,\n    x="x",\n    y="y",\n    z="z",\n    color="cluster",\n    hover_data=["title"],\n    width=1000,\n    height=1000,\n    size_max=5,\n)\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"fig.show()\n")),(0,r.kt)("h2",{id:"exercises"},"Exercises"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Adapt the data-fetching code to use the API, instead of the works snapshot."),(0,r.kt)("li",{parentName:"ol"},"Use a different pre-trained model to extract features from the text. How does the visualisation change?"),(0,r.kt)("li",{parentName:"ol"},"Try using a different clustering algorithm to cluster the works. How do the meanings/boundaries of the clusters change?"),(0,r.kt)("li",{parentName:"ol"},"Try to build a simple semantic search function, by allowing the user to enter a search term, embedding their search term using the feature-extracting model, and returning the titles that are closest to that term. How well does it work? How could you improve it?")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"")))}u.isMDXComponent=!0},8209:(e,t,n)=>{n(7294)}}]);