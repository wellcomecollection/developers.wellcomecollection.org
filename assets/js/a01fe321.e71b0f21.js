"use strict";(self.webpackChunkdevelopers=self.webpackChunkdevelopers||[]).push([[347],{3905:(e,t,n)=>{n.d(t,{Zo:()=>i,kt:()=>d});var s=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);t&&(s=s.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,s)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,s,r=function(e,t){if(null==e)return{};var n,s,r={},o=Object.keys(e);for(s=0;s<o.length;s++)n=o[s],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(s=0;s<o.length;s++)n=o[s],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=s.createContext({}),u=function(e){var t=s.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},i=function(e){var t=u(e.components);return s.createElement(c.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return s.createElement(s.Fragment,{},t)}},g=s.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,i=l(e,["components","mdxType","originalType","parentName"]),g=u(n),d=r,m=g["".concat(c,".").concat(d)]||g[d]||p[d]||o;return n?s.createElement(m,a(a({ref:t},i),{},{components:n})):s.createElement(m,a({ref:t},i))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,a=new Array(o);a[0]=g;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:r,a[1]=l;for(var u=2;u<o;u++)a[u]=n[u];return s.createElement.apply(null,a)}return s.createElement.apply(null,n)}g.displayName="MDXCreateElement"},1545:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>u});var s=n(7462),r=(n(7294),n(3905));n(8209);const o={},a="2. Extracting more data for local analysis",l={unversionedId:"examples/extracting-more-data-for-local-analysis",id:"examples/extracting-more-data-for-local-analysis",title:"2. Extracting more data for local analysis",description:"View on GitHub | Run in Google Colab",source:"@site/docs/examples/02-extracting-more-data-for-local-analysis.md",sourceDirName:"examples",slug:"/examples/extracting-more-data-for-local-analysis",permalink:"/docs/examples/extracting-more-data-for-local-analysis",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"1. Exploring Wellcome Collection's APIs",permalink:"/docs/examples/exploring-wellcome-collections-apis"},next:{title:"3. Connecting the APIs together",permalink:"/docs/examples/connecting-the-apis-together"}},c={},u=[{value:"2.1 Page sizes",id:"21-page-sizes",level:2},{value:"2.2 Requesting multiple pages of results",id:"22-requesting-multiple-pages-of-results",level:2},{value:"2.3 Analyzing our two sets of results",id:"23-analyzing-our-two-sets-of-results",level:2},{value:"2.4 Creating a generic function for finding subject intersections",id:"24-creating-a-generic-function-for-finding-subject-intersections",level:2},{value:"Exercises",id:"exercises",level:2}],i={toc:u};function p(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,s.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"2-extracting-more-data-for-local-analysis"},"2. Extracting more data for local analysis"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/wellcomecollection/developers.wellcomecollection.org/tree/HEAD/notebooks/02-extracting-more-data-for-local-analysis.ipynb"},"View on GitHub")," | ",(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/wellcomecollection/developers.wellcomecollection.org/tree/HEAD/notebooks/02-extracting-more-data-for-local-analysis.ipynb"},"Run in Google Colab")),(0,r.kt)("p",null,"In the last notebook, we saw that the ",(0,r.kt)("inlineCode",{parentName:"p"},"/works")," API can do some clever querying and filtering. However, we often have questions which can't be answered by the API by itself. In those cases, it's useful to collect a load of data from the API and then analyse it locally."),(0,r.kt)("p",null,"In this notebook, we'll try to query the API for bigger chunks of data so that we can answer a more interesting question."),(0,r.kt)("p",null,"We'll aim to find out:"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"If we filter the works API for a set of subjects, can we find the other subjects that most commonly co-occur with them?")),(0,r.kt)("p",null,"We'll start by fetching all of the works which are tagged with a single subject."),(0,r.kt)("p",null,"Here's our base URL again:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'base_url = "https://api.wellcomecollection.org/catalogue/v2/"\n')),(0,r.kt)("p",null,'Lets\' make a request to the API, asking for all the works which are tagged with the subject "Influenza".'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import requests\n\nresponse = requests.get(\n    base_url + "works", params={"subjects.label": "Influenza"}\n).json()\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response["totalResults"]\n')),(0,r.kt)("h2",{id:"21-page-sizes"},"2.1 Page sizes"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response["totalPages"]\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'len(response["results"])\n')),(0,r.kt)("p",null,"At the moment, we're getting our results spread across 9 pages, because ",(0,r.kt)("inlineCode",{parentName:"p"},"pageSize")," is set to 10 by default. "),(0,r.kt)("p",null,"We can increase the ",(0,r.kt)("inlineCode",{parentName:"p"},"pageSize")," to get all of our 81 works in one go (up to a maximum of 100):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import requests\n\nresponse = requests.get(\n    base_url + "works", params={"subjects.label": "Influenza", "pageSize": 100}\n).json()\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response["totalResults"]\n\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'response["totalPages"]\n')),(0,r.kt)("h2",{id:"22-requesting-multiple-pages-of-results"},"2.2 Requesting multiple pages of results"),(0,r.kt)("p",null,"Some subjects only appear on a few works, but others appear on thousands. If we want to be able to analyse those larger subjects, we'll need to fetch more than 100 works at a time. To do this, we'll page through the results, making multiple requests and building a local list of results as we go."),(0,r.kt)("p",null,"If the API finds more than one page of results for a query, it will provide a ",(0,r.kt)("inlineCode",{parentName:"p"},"nextPage")," field in the response, with a link to the next page of results. We can use this to fetch the next page of results, and the next, and the next, until the ",(0,r.kt)("inlineCode",{parentName:"p"},"nextPage")," field is no longer present, at which point we know we've got all the results."),(0,r.kt)("p",null,"We're going to use these results to answer our question from the introduction, so we'll also ask the API to include the subjects which are associated with each work, and collect them too."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"from tqdm.auto import tqdm\n\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'results = []\n\n# fetch the first page of results\nresponse = requests.get(\n    base_url + "works",\n    params={\n        "subjects.label": "England",\n        "include": "subjects",\n        "pageSize": "100",\n    },\n).json()\n\n# start a progress bar to keep track of how many results we\'ve fetched\nprogress_bar = tqdm(total=response["totalResults"])\n\n# add our results to the list and update our progress bar\nresults.extend(response["results"])\nprogress_bar.update(len(response["results"]))\n\n# as long as there\'s a "nextPage" key in the response, keep fetching results\n# adding them to the list, and updating the progress bar\nwhile "nextPage" in response:\n    response = requests.get(response["nextPage"]).json()\n    results.extend(response["results"])\n    progress_bar.update(len(response["results"]))\n\nprogress_bar.close()\n\nworks_about_england = results\n')),(0,r.kt)("p",null,"let's check that we've got the correct number of results:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'len(works_about_england) == response["totalResults"]\n')),(0,r.kt)("p",null,"Great! Now let's try collecting works for a second subject:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'results = []\n\nresponse = requests.get(\n    base_url + "works",\n    params={\n        "subjects.label": "Germany",\n        "include": "subjects",\n        "pageSize": "100",\n    },\n).json()\n\nprogress_bar = tqdm(total=response["totalResults"])\n\nresults.extend(response["results"])\nprogress_bar.update(len(response["results"]))\n\nwhile "nextPage" in response:\n    response = requests.get(response["nextPage"]).json()\n    results.extend(response["results"])\n    progress_bar.update(len(response["results"]))\n\nprogress_bar.close()\n\nworks_about_germany = results\n')),(0,r.kt)("h2",{id:"23-analyzing-our-two-sets-of-results"},"2.3 Analyzing our two sets of results"),(0,r.kt)("p",null,"Let's find the works which are tagged with both subjects by filtering the results of the first list by IDs from the second list."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'ids_from_works_about_england = set([work["id"] for work in works_about_england])\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'works_about_england_and_germany = [\n    work\n    for work in works_about_germany\n    if work["id"] in ids_from_works_about_england\n]\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"len(works_about_england_and_germany)\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"works_about_england_and_germany\n")),(0,r.kt)("p",null,"That's 32 works which are tagged with both ",(0,r.kt)("inlineCode",{parentName:"p"},"England")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Germany"),". Let's see if we can find the other subjects which are most commonly found on these works. "),(0,r.kt)("p",null,"Let's use a ",(0,r.kt)("inlineCode",{parentName:"p"},"Counter")," to figure that out:"),(0,r.kt)("p",null,"N.B. We're collecting the ",(0,r.kt)("em",{parentName:"p"},"concepts"),' on each work because they are the atomic constituent parts of subjects. Our catalogue includes subjects like "Surgery - 18th Century" which are made up of the concepts "Surgery" and "18th Century". It\'s more desirable to compare the concepts, because the subjects can be so specific and are less likely to overlap.'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from collections import Counter\n\nconcepts = Counter()\n\nfor record in works_about_england_and_germany:\n    # we need to navigate the nested structure of the subject and its concepts to\n    # get the complete list of _concepts_ on each work\n    for subject in record["subjects"]:\n        for concept in subject["concepts"]:\n            concepts.update([concept["label"]])\n')),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Counter")," object keeps track of the counts of each unique item we pass to it. Now that we've added the complete list, we can ask it for the most common items:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"concepts.most_common(20)\n")),(0,r.kt)("p",null,"Great! We've solved our original problem:"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"If we filter the works API for a set of subjects, can we find the other concepts that most commonly co-occur with them?")),(0,r.kt)("h2",{id:"24-creating-a-generic-function-for-finding-subject-intersections"},"2.4 Creating a generic function for finding subject intersections"),(0,r.kt)("p",null,"Now that we've solved this problem, let's try to make it more generic so that we can use it for other pairs of subjects."),(0,r.kt)("p",null,"We can re-use a lot of the code we've already written, and wrap it in a couple of reusable function definitions."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'def get_subject_results(subject):\n    response = requests.get(\n        base_url + "works",\n        params={\n            "subjects.label": subject,\n            "include": "subjects",\n            "pageSize": "100",\n        },\n    ).json()\n\n    progress_bar = tqdm(total=response["totalResults"])\n    results = response["results"]\n    progress_bar.update(len(response["results"]))\n\n    while "nextPage" in response:\n        response = requests.get(response["nextPage"]).json()\n        results.extend(response["results"])\n        progress_bar.update(len(response["results"]))\n\n    progress_bar.close()\n    \n    return results\n\n\ndef find_intersecting_subject_concepts(subject_1, subject_2, n=20):\n    subject_1_results = get_subject_results(subject_1)\n    subject_2_results = get_subject_results(subject_2)\n    subject_2_ids = set(result["id"] for result in subject_2_results)\n\n    intersecting_results = [\n        result for result in subject_1_results if result["id"] in subject_2_ids\n    ]\n\n    concepts = Counter()\n    for record in intersecting_results:\n        for subject in record["subjects"]:\n            for concept in subject["concepts"]:\n                concepts.update([concept["label"]])\n\n    return concepts.most_common(n)\n')),(0,r.kt)("p",null,"Calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"find_intersecting_subject_concepts()")," function with any two subjects will return a counter of the most common concepts found on the works which are tagged with both subjects."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'find_intersecting_subject_concepts("Europe", "United States")\n')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'find_intersecting_subject_concepts("Vomiting", "Witchcraft")\n')),(0,r.kt)("h2",{id:"exercises"},"Exercises"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Try running the function with different subjects. Use the API to find two subjects which appear on a few hundred or a few thousand works, and see if you can find the most common concepts which appear on both of them."),(0,r.kt)("li",{parentName:"ol"},"Adapt the code to compare an arbitrary number of subjects, rather than just two.")))}p.isMDXComponent=!0},8209:(e,t,n)=>{n(7294)}}]);